{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Built-In Web App\n",
    "\n",
    "**OnPrem.LLM** includes a built-in Web app to access the LLM. \n",
    "\n",
    "To start it, run the following command after installation:\n",
    "\n",
    "```shell\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"600\"/>\n",
    "\n",
    "More information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\r\n",
    "\r\n",
    "Start the OnPrem.LLM web app\r\n",
    "Example: onprem --port 8000\r\n",
    "\r\n",
    "optional arguments:\r\n",
    "  -h, --help            show this help message and exit\r\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\r\n",
    "  -a ADDRESS, --address ADDRESS\r\n",
    "                        Address to bind; default is 0.0.0.0\r\n",
    "  -v, --version         Print a version\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `webapp.yml` exists in the `onprem_data` folder in the user's home directory. This file stores information used by the Web app such as the model to use. An example `webapp.yml` is [here](https://github.com/amaiya/onprem/blob/master/nbs/webapp.yml) and is also shown below:\n",
    "\n",
    "```yaml\n",
    "# sample webapp.yml\n",
    "llm:\r\n",
    "  # model url (or model file name if previously downloaded)\r\n",
    "  model_url: https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGML/resolve/main/wizardlm-13b-v1.2.ggmlv3.q4_0.bin\r\n",
    "  # number of layers offloaded to GPU\r\n",
    "  n_gpu_layers: 32\r\n",
    "  # path to vector db folder\r\n",
    "  vectordb_path: your_usernameamaiya/onprem_data/vectordb\r\n",
    "  # path to model download folder\r\n",
    "  model_download_pathyour_usernamee/amaiya/onprem_data\r\n",
    "prompt:\r\n",
    "  # text to append to end of prompt\r\n",
    "  append_to_prompt: Do not answer if there is no mention of this in context.\r\n",
    "streamlit:\r\n",
    "  # title of application\r\n",
    "  title: OnPrem.LLM\r\n",
    "  # subtitle in \"Talk to Your Documents\" screen\r\n",
    "  rag_title:\r\n",
    "  # number of used documents to use by LLM.ask and LLM.chat\r\n",
    "  num_source_docs: 4\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
