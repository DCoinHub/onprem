{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Built-In Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**OnPrem.LLM** includes a built-in Web app to access the LLM. \n",
    "\n",
    "#### Step 1: Ingest some documents using the Python API:\n",
    "```python\n",
    "# run at Python prompt\n",
    "from onprem import LLM\n",
    "llm = LLM()\n",
    "llm.ingest('/your/folder/of/documents')\n",
    "```\n",
    "\n",
    "#### Step 2: Start the Web app:\n",
    "\n",
    "```shell\n",
    "# run at command-line\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"800\"/>\n",
    "\n",
    "The Web app is implemented with [streamlit](https://streamlit.io/): `pip install streamlit`.  If it is not already installed, the `onprem` command will ask you to install it.\n",
    "More information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\n",
    "\n",
    "Start the OnPrem.LLM web app\n",
    "Example: onprem --port 8000\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\n",
    "  -a ADDRESS, --address ADDRESS\n",
    "                        Address to bind; default is 0.0.0.0\n",
    "  -v, --version         Print a version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `webapp.yml` exists in the `onprem_data` folder in the user's home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below:\n",
    "\n",
    "```yaml\n",
    "llm:\n",
    "  # model url (or model file name if previously downloaded)\n",
    "  model_url: https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf\n",
    "  # number of layers offloaded to GPU\n",
    "  n_gpu_layers: 32\n",
    "  # path to vector db folder\n",
    "  vectordb_path: /home/your_user_name/onprem_data/vectordb\n",
    "  # path to model download folder\n",
    "  model_download_path: /home/your_user_name/onprem_data\n",
    "  # number of source documents used by LLM.ask and LLM.chat\n",
    "  rag_num_source_docs: 6\n",
    "  # minimum similarity score for a source to be used bi LLM.ask and LLM.chat\n",
    "  rag_score_threshold: 0.0\n",
    "ui:\n",
    "  # title of application\n",
    "  title: OnPrem.LLM\n",
    "  # subtitle in \"Talk to Your Documents\" screen\n",
    "  rag_title:\n",
    "  # path to folder containing raw documents (used to construct direct links to document sources)\n",
    "  rag_source_path:\n",
    "  # base url (used to construct direct links to document sources)\n",
    "  rag_base_url:\n",
    "```\n",
    "\n",
    "You can edit the file based on your requirements.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/cat_names.png\" border=\"1\" alt=\"screenshot\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
