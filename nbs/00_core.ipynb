{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Core functionality for `onprem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GPT4All, LlamaCpp\n",
    "import chromadb\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from onprem import utils as U\n",
    "DEFAULT_MODEL_URL = 'https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin'\n",
    "DEFAULT_MODEL_NAME = os.path.basename(DEFAULT_MODEL_URL)\n",
    "DEFAULT_EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, \n",
    "                 model_name=DEFAULT_MODEL_NAME,\n",
    "                 n_gpu_layers:Optional[int]=None, \n",
    "                 max_tokens:int=512, \n",
    "                 n_ctx:int=2048, \n",
    "                 n_batch:int=1024,\n",
    "                 mute_stream=False,\n",
    "                 embedding_model_name:str ='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                 embedding_model_kwargs:dict ={'device': 'cpu'},\n",
    "                verbose=False):\n",
    "        \"\"\"\n",
    "        LLM Constructor\n",
    "        \n",
    "        **Args:**\n",
    "\n",
    "        - *model_name*: Name of the model.  Must downloaded with call `LLM.download_model`.\n",
    "        - *n_gpu_layers*: Number of layers to be loaded into gpu memory. Default is `None`.\n",
    "        - *max_tokens*: The maximum number of tokens to generate.\n",
    "        - *n_ctx*: Token context window.\n",
    "        - *n_batch*: Number of tokens to process in parallel.\n",
    "        - *mute_stream*: Mute ChatGPT-like token stream output during generation\n",
    "        - *embedding_model*: name of sentence-transformers model. Used for `LLM.ingest` and `LLM.ask`.\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`).\n",
    "        - *verbose*: Verbosity\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        if not os.path.isfile(os.path.join(U.get_datadir(), model_name)):\n",
    "            warnings.warn('The model {model_name} does not exist in {U.get_datadir()}. '+\\\n",
    "                          'Please execute LLM.download_model() to download it.')\n",
    "        self.llm = None\n",
    "        self.ingester = None\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.max_tokens = max_tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_batch = n_batch\n",
    "        self.callbacks = [] if mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_kwargs = embedding_model_kwargs\n",
    "        self.verbose = verbose\n",
    " \n",
    "    @classmethod\n",
    "    def download_model(cls, model_url=DEFAULT_MODEL_URL, confirm=True, ssl_verify=True):\n",
    "        \"\"\"\n",
    "        Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "        - *model_url*: URL of model\n",
    "        - *confirm*: whether or not to confirm with user before downloading\n",
    "        - *ssl_verify*: If True, SSL certificates are verified. \n",
    "                        You can set to False if corporate firewall gives you problems.\n",
    "        \"\"\"\n",
    "        datadir = U.get_datadir()\n",
    "        model_name = os.path.basename(model_url)\n",
    "        filename = os.path.join(datadir, model_name)\n",
    "        confirm_msg = f\"You are about to download the LLM {model_name} to the {datadir} folder. Are you sure?\"\n",
    "        if os.path.isfile(filename):\n",
    "            confirm_msg = f'There is already a file {model_name} in {datadir}.\\n Do you want to still download it?'\n",
    "            \n",
    "        shall = True\n",
    "        if confirm:\n",
    "            shall = input(\"%s (Y/n) \" % confirm_msg) == \"Y\"\n",
    "        if shall:\n",
    "            U.download(model_url, filename, verify=ssl_verify)\n",
    "        else:\n",
    "            warnings.warn(f'{model_name} was not downloaded because \"Y\" was not selected.')\n",
    "        return\n",
    "\n",
    "    def load_ingester(self):\n",
    "        \"\"\"Get Ingester instance\"\"\"\n",
    "        if not self.ingester:\n",
    "            from onprem.ingest import Ingester\n",
    "            self.ingester = Ingester(embedding_model_name=self.embedding_model_name,\n",
    "                                     embedding_model_kwargs=self.embedding_model_kwargs)\n",
    "        return self.ingester\n",
    "        \n",
    "        \n",
    "    def ingest(self, \n",
    "               source_directory:str,\n",
    "              ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` into vector database.\n",
    "        Previously-ingested documents are ignored.\n",
    "\n",
    "        **Args:**\n",
    "        \n",
    "        - *source_directory*: path to folder containing document store\n",
    "\n",
    "        \n",
    "        **Returns:** `None`\n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        ingester.ingest(source_directory)\n",
    "        return\n",
    "\n",
    " \n",
    "        \n",
    "    def check_model(self):\n",
    "        datadir = U.get_datadir()\n",
    "        model_path = os.path.join(datadir, self.model_name)\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise ValueError(f'The LLM model {self.model_name} does not appear to have been downloaded. '+\\\n",
    "                             f'Execute the download_model() method to download it.')\n",
    "        return model_path\n",
    "        \n",
    " \n",
    "    def load_llm(self):\n",
    "        model_path = self.check_model()\n",
    "        \n",
    "        if not self.llm:\n",
    "            self.llm =  llm = LlamaCpp(model_path=model_path, \n",
    "                                       max_tokens=self.max_tokens, \n",
    "                                       n_batch=self.n_batch, \n",
    "                                       callbacks=self.callbacks, \n",
    "                                       verbose=self.verbose, \n",
    "                                       n_gpu_layers=self.n_gpu_layers, \n",
    "                                       n_ctx=self.n_ctx)    \n",
    "\n",
    "        return self.llm\n",
    "        \n",
    "        \n",
    "    def prompt(self, prompt):\n",
    "        \"\"\"\n",
    "        Send prompt to LLM to generate a response\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        return llm(prompt)  \n",
    "    \n",
    "    def ask(self, question, num_source_docs=4):\n",
    "        \"\"\"\n",
    "        Answer a question based on source documents fed to the `ingest` method.\n",
    "        \n",
    "        **Args:**\n",
    "        - question: a question you want to ask\n",
    "        - num_source_docs: the number of ingested source documents use to generate answer\n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        db = ingester.get_db()\n",
    "        if not db:\n",
    "            raise ValueError('A vector database has not yet been created. Please call the LLM.ingest method.')\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": num_source_docs})\n",
    "        llm = self.load_llm()\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                         chain_type=\"stuff\", \n",
    "                                         retriever=retriever, \n",
    "                                         return_source_documents= True)\n",
    "        res = qa(question)\n",
    "        return res['result'], res['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L68){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url='https://huggingface.co/TheBloke/Wizard-\n",
       ">                          Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.ggmlv3.q4_0.bin', confirm=True,\n",
       ">                          ssl_verify=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L68){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url='https://huggingface.co/TheBloke/Wizard-\n",
       ">                          Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.ggmlv3.q4_0.bin', confirm=True,\n",
       ">                          ssl_verify=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model_name=DEFAULT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile( os.path.join(U.get_datadir(), DEFAULT_MODEL_NAME) ):\n",
    "    LLM.download_model(DEFAULT_MODEL_URL, confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(os.path.join(U.get_datadir(), DEFAULT_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt)\n",
       "\n",
       "Send prompt to LLM to generate a response"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt)\n",
       "\n",
       "Send prompt to LLM to generate a response"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/amaiya/onprem_data/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5407.72 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L104){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "\n",
       "**Returns:** `None`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L104){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "\n",
       "**Returns:** `None`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from ./sample_data\n",
      "Split into 62 chunks of text (max. 500 tokens each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the prompt method\n"
     ]
    }
   ],
   "source": [
    "llm.ingest('./sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question, num_source_docs=4)\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "- question: a question you want to ask\n",
       "- num_source_docs: the number of ingested source documents use to generate answer"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/core.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question, num_source_docs=4)\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "- question: a question you want to ask\n",
       "- num_source_docs: the number of ingested source documents use to generate answer"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/amaiya/onprem_data/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5407.72 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhat is ktrain in a single sentence?\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \n\u001b[0;32m----> 2\u001b[0m answer, docs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReferences:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs):\n",
      "Cell \u001b[0;32mIn[29], line 165\u001b[0m, in \u001b[0;36mLLM.ask\u001b[0;34m(self, question, num_source_docs)\u001b[0m\n\u001b[1;32m    159\u001b[0m chain_type_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: PROMPT}\n\u001b[1;32m    160\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mllm, \n\u001b[1;32m    161\u001b[0m                                  chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    162\u001b[0m                                  retriever\u001b[38;5;241m=\u001b[39mretriever, \n\u001b[1;32m    163\u001b[0m                                  return_source_documents\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m                                 chain_type_kwargs\u001b[38;5;241m=\u001b[39mchain_type_kwargs)\n\u001b[0;32m--> 165\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/langchain/chains/base.py:435\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute chain when there's a single string output.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03mThe main difference between this method and `Chain.__call__` is that this method\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m        # -> \"The temperature in Boise is...\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Run at start to make sure this is possible/defined\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m _output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_output_key\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/langchain/chains/base.py:380\u001b[0m, in \u001b[0;36mChain._run_output_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_output_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` not supported when there is not exactly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output key. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got ['result', 'source_documents']."
     ]
    }
   ],
   "source": [
    "question = \"\"\"What is ktrain in a single sentence?\"\"\" \n",
    "answer, docs = llm.ask(question)\n",
    "print('\\n\\nReferences:\\n\\n')\n",
    "for i, document in enumerate(docs):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
