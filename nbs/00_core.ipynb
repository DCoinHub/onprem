{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Core functionality for `onprem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import chromadb\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from onprem import utils as U\n",
    "DEFAULT_MODEL_URL = 'https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin'\n",
    "DEFAULT_LARGER_URL = ' https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGML/resolve/main/wizardlm-13b-v1.2.ggmlv3.q4_0.bin'\n",
    "DEFAULT_EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "DEFAULT_QA_PROMPT = \"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, \n",
    "                 model_url=DEFAULT_MODEL_URL,\n",
    "                 use_larger:bool=False,\n",
    "                 n_gpu_layers:Optional[int]=None, \n",
    "                 model_download_path:Optional[str]=None,\n",
    "                 vectordb_path:Optional[str]=None,\n",
    "                 max_tokens:int=512, \n",
    "                 n_ctx:int=2048, \n",
    "                 n_batch:int=1024,\n",
    "                 mute_stream:bool=False,\n",
    "                 embedding_model_name:str ='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                 embedding_model_kwargs:dict ={'device': 'cpu'},\n",
    "                 embedding_encode_kwargs:dict ={'normalize_embeddings': False},\n",
    "                 confirm:bool=True,\n",
    "                 verbose:bool=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        LLM Constructor.  Extra `kwargs` are fed directly to `langchain.llms.LlamaCpp`.\n",
    "        \n",
    "        **Args:**\n",
    "\n",
    "        - *model_url*: URL to `.bin` model (currently must be GGML model).\n",
    "        - *use_larger*: If True, a larger model than the default `model_url` will be used.\n",
    "        - *n_gpu_layers*: Number of layers to be loaded into gpu memory. Default is `None`.\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *vectordb_path*: Path to vector database (created if it doesn't exist). \n",
    "                           Default is `onprem_data/vectordb` in user's home directory.\n",
    "        - *max_tokens*: The maximum number of tokens to generate.\n",
    "        - *n_ctx*: Token context window.\n",
    "        - *n_batch*: Number of tokens to process in parallel.\n",
    "        - *mute_stream*: Mute ChatGPT-like token stream output during generation\n",
    "        - *embedding_model_name*: name of sentence-transformers model. Used for `LLM.ingest` and `LLM.ask`.\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`).\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of \n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        - *confirm*: whether or not to confirm with user before downloading a model\n",
    "        - *verbose*: Verbosity\n",
    "        \"\"\"\n",
    "        self.model_url = DEFAULT_LARGER_URL if use_larger else model_url\n",
    "        if verbose:\n",
    "            print(f'Since use_larger=True, we are using: {os.path.basename(DEFAULT_LARGER_URL)}')\n",
    "        self.model_name = os.path.basename(self.model_url)\n",
    "        self.model_download_path = model_download_path or U.get_datadir()\n",
    "        if not os.path.isfile(os.path.join(self.model_download_path, self.model_name)):\n",
    "            self.download_model(self.model_url, model_download_path=self.model_download_path, confirm=confirm)\n",
    "        self.vectordb_path = vectordb_path\n",
    "        self.llm = None\n",
    "        self.ingester = None\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.max_tokens = max_tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_batch = n_batch\n",
    "        self.callbacks = [] if mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_kwargs = embedding_model_kwargs\n",
    "        self.embedding_encode_kwargs = embedding_encode_kwargs\n",
    "        self.verbose = verbose\n",
    "        self.extra_kwargs = kwargs\n",
    " \n",
    "    @classmethod\n",
    "    def download_model(cls, model_url:str=DEFAULT_MODEL_URL, \n",
    "                       model_download_path:Optional[str]=None, \n",
    "                       confirm:bool=True, \n",
    "                       ssl_verify:bool=True):\n",
    "        \"\"\"\n",
    "        Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "        - *model_url*: URL of model\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *confirm*: whether or not to confirm with user before downloading\n",
    "        - *ssl_verify*: If True, SSL certificates are verified. \n",
    "                        You can set to False if corporate firewall gives you problems.\n",
    "        \"\"\"\n",
    "        datadir = model_download_path or U.get_datadir()\n",
    "        model_name = os.path.basename(model_url)\n",
    "        filename = os.path.join(datadir, model_name)\n",
    "        confirm_msg = f\"You are about to download the LLM {model_name} to the {datadir} folder. Are you sure?\"\n",
    "        if os.path.isfile(filename):\n",
    "            confirm_msg = f'There is already a file {model_name} in {datadir}.\\n Do you want to still download it?'\n",
    "            \n",
    "        shall = True\n",
    "        if confirm:\n",
    "            shall = input(\"%s (y/N) \" % confirm_msg).lower() == \"y\"\n",
    "        if shall:\n",
    "            U.download(model_url, filename, verify=ssl_verify)\n",
    "        else:\n",
    "            warnings.warn(f'{model_name} was not downloaded because \"Y\" was not selected.')\n",
    "        return\n",
    "\n",
    "    def load_ingester(self):\n",
    "        \"\"\"\n",
    "        Get `Ingester` instance. \n",
    "        You can access the `langchain.vectorstores.Chroma` instance with `load_ingester().get_db()`.\n",
    "        \"\"\"\n",
    "        if not self.ingester:\n",
    "            from onprem.ingest import Ingester\n",
    "            self.ingester = Ingester(embedding_model_name=self.embedding_model_name,\n",
    "                                     embedding_model_kwargs=self.embedding_model_kwargs,\n",
    "                                     embedding_encode_kwargs=self.embedding_encode_kwargs,\n",
    "                                     persist_directory=self.vectordb_path)\n",
    "        return self.ingester\n",
    "        \n",
    "        \n",
    "    def ingest(self, \n",
    "               source_directory:str,\n",
    "               chunk_size:int=500,\n",
    "               chunk_overlap:int=50\n",
    "              ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` into vector database.\n",
    "        Previously-ingested documents are ignored.\n",
    "\n",
    "        **Args:**\n",
    "        \n",
    "        - *source_directory*: path to folder containing document store\n",
    "        - *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        - *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        \n",
    "        **Returns:** `None`\n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        ingester.ingest(source_directory, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        return\n",
    "\n",
    " \n",
    "        \n",
    "    def check_model(self):\n",
    "        \"\"\"\n",
    "        Returns the path to the model\n",
    "        \"\"\"\n",
    "        datadir = self.model_download_path\n",
    "        model_path = os.path.join(datadir, self.model_name)\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise ValueError(f'The LLM model {self.model_name} does not appear to have been downloaded. '+\\\n",
    "                             f'Execute the download_model() method to download it.')\n",
    "        return model_path\n",
    "        \n",
    " \n",
    "    def load_llm(self):\n",
    "        \"\"\"\n",
    "        Loads the LLM from the model path.\n",
    "        \"\"\"\n",
    "        model_path = self.check_model()\n",
    "        \n",
    "        if not self.llm:\n",
    "            self.llm =  llm = LlamaCpp(model_path=model_path, \n",
    "                                       max_tokens=self.max_tokens, \n",
    "                                       n_batch=self.n_batch, \n",
    "                                       callbacks=self.callbacks, \n",
    "                                       verbose=self.verbose, \n",
    "                                       n_gpu_layers=self.n_gpu_layers, \n",
    "                                       n_ctx=self.n_ctx, **self.extra_kwargs)    \n",
    "\n",
    "        return self.llm\n",
    "        \n",
    "        \n",
    "    def prompt(self, prompt, prompt_template:Optional[str]=None):\n",
    "        \"\"\"\n",
    "        Send prompt to LLM to generate a response\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "        - *prompt*: The prompt to supply to the model\n",
    "        - *prompt_template*: Optional prompt template (must have a variable named \"prompt\")\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        if prompt_template:\n",
    "            prompt = prompt_template.format(**{'prompt': prompt})\n",
    "        return llm(prompt)  \n",
    " \n",
    "\n",
    "    def load_qa(self, num_source_docs:int=4, prompt_template:str=DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Prepares and loads the `langchain.chains.RetrievalQA` object\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "        - *num_source_docs*: the number of ingested source documents use to generate answer\n",
    "        - *prompt_template*: A string representing the prompt with variables \"context\" and \"question\"      \n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        db = ingester.get_db()\n",
    "        if not db:\n",
    "            raise ValueError('A vector database has not yet been created. Please call the LLM.ingest method.')\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": num_source_docs})\n",
    "        llm = self.load_llm()\n",
    "        PROMPT = PromptTemplate(\n",
    "                    template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                         chain_type=\"stuff\", \n",
    "                                         retriever=retriever, \n",
    "                                         return_source_documents= True,\n",
    "                                         chain_type_kwargs={'prompt':PROMPT})\n",
    "        return qa\n",
    "\n",
    "    \n",
    "    def ask(self, question:str, num_source_docs:int=4, prompt_template=DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Answer a question based on source documents fed to the `ingest` method.\n",
    "        \n",
    "        **Args:**\n",
    "        \n",
    "        - *question*: a question you want to ask\n",
    "        - *num_source_docs*: the number of ingested source documents use to generate answer\n",
    "        - *prompt_template*: A string representing the prompt with variables \"context\" and \"question\"\n",
    "        \"\"\"\n",
    "        qa = self.load_qa(num_source_docs=num_source_docs, prompt_template=prompt_template)\n",
    "        res = qa(question)\n",
    "        return res['result'], res['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L92){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url='https://huggingface.co/TheBloke/Wizard-\n",
       ">                          Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.ggmlv3.q4_0.bin',\n",
       ">                          model_download_path:Optional[str]=None, confirm=True,\n",
       ">                          ssl_verify=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L92){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url='https://huggingface.co/TheBloke/Wizard-\n",
       ">                          Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.ggmlv3.q4_0.bin',\n",
       ">                          model_download_path:Optional[str]=None, confirm=True,\n",
       ">                          ssl_verify=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "Loads the LLM from the model path."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "Loads the LLM from the model path."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_ingester\n",
       "\n",
       ">      LLM.load_ingester ()\n",
       "\n",
       "Get `Ingester` instance. \n",
       "You can access the `langchain.vectorstores.Chroma` instance with `load_ingester().get_db()`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_ingester\n",
       "\n",
       ">      LLM.load_ingester ()\n",
       "\n",
       "Get `Ingester` instance. \n",
       "You can access the `langchain.vectorstores.Chroma` instance with `load_ingester().get_db()`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_ingester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_qa\n",
       "\n",
       ">      LLM.load_qa (num_source_docs:int=4, prompt_template='\"Use the following\n",
       ">                   pieces of context to answer the question at the end. If you\n",
       ">                   don\\'t know the answer, just say that you don\\'t know,\n",
       ">                   don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion:\n",
       ">                   {question}\\nHelpful Answer:')\n",
       "\n",
       "Prepares and loads the `langchain.chains.RetrievalQA` object\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *num_source_docs*: the number of ingested source documents use to generate answer\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_qa\n",
       "\n",
       ">      LLM.load_qa (num_source_docs:int=4, prompt_template='\"Use the following\n",
       ">                   pieces of context to answer the question at the end. If you\n",
       ">                   don\\'t know the answer, just say that you don\\'t know,\n",
       ">                   don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion:\n",
       ">                   {question}\\nHelpful Answer:')\n",
       "\n",
       "Prepares and loads the `langchain.chains.RetrievalQA` object\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *num_source_docs*: the number of ingested source documents use to generate answer\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt, prompt_template=None)\n",
       "\n",
       "Send prompt to LLM to generate a response"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt, prompt_template=None)\n",
       "\n",
       "Send prompt to LLM to generate a response"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "- *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "- *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns:** `None`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "- *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "- *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns:** `None`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, num_source_docs:int=4, prompt_template='\"Use the\n",
       ">               following pieces of context to answer the question at the end.\n",
       ">               If you don\\'t know the answer, just say that you don\\'t know,\n",
       ">               don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion:\n",
       ">               {question}\\nHelpful Answer:')\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "- *num_source_docs*: the number of ingested source documents use to generate answer\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, num_source_docs:int=4, prompt_template='\"Use the\n",
       ">               following pieces of context to answer the question at the end.\n",
       ">               If you don\\'t know the answer, just say that you don\\'t know,\n",
       ">               don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion:\n",
       ">               {question}\\nHelpful Answer:')\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "- *num_source_docs*: the number of ingested source documents use to generate answer\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(DEFAULT_MODEL_URL, confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(os.path.join(U.get_datadir(), os.path.basename(DEFAULT_MODEL_URL))), \"missing model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA TITAN V, compute capability 7.0\n",
      "  Device 1: NVIDIA TITAN V, compute capability 7.0\n",
      "llama.cpp: loading model from /home/amaiya/onprem_data/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA TITAN V) as main device\n",
      "llama_model_load_internal: mem required  = 5407.72 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 384 MB\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert saved_output.strip() == 'Cillian Murphy, Florence Pugh', \"bad response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 12:31:37.712882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from ./sample_data\n",
      "Split into 62 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "llm.ingest('./sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ktrain is a low-code library that automates various aspects of machine learning, enabling beginners and domain experts to perform tasks without extensive programming or data science experience. It was inspired by other low-code ML libraries like fastai and ludwig, which aim to democratize machine learning.\n",
      "\n",
      "References:\n",
      "\n",
      "\n",
      "\n",
      "1.> ./sample_data/ktrain_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "2.> ./sample_data/ktrain_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best ﬁt their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n",
      "\n",
      "3.> ./sample_data/ktrain_paper.pdf:\n",
      "with custom models and data formats, as well.\n",
      "Inspired by other low-code (and no-\n",
      "code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n",
      "(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\n",
      "enabling beginners and domain experts with minimal programming or data science experi-\n",
      "4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
      "6\n",
      "\n",
      "4.> ./sample_data/ktrain_paper.pdf:\n",
      "ktrain: A Low-Code Library for Augmented Machine Learning\n",
      "toML platform and more of what might be called a “low-code” ML platform. Through\n",
      "automation or semi-automation, ktrain facilitates the full machine learning workﬂow from\n",
      "curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\n",
      "tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\n",
      "experts who may have less experience with machine learning and software coding. Where\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "question = \"\"\"What is ktrain? Remember to only use the provided context.\"\"\" \n",
    "answer, docs = llm.ask(question)\n",
    "print('\\n\\nReferences:\\n\\n')\n",
    "for i, document in enumerate(docs):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-Tip**: If you see the model hallucinating answers, you can supply `use_larger=True` to `LLM` and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to `LLM`), which can provide better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
