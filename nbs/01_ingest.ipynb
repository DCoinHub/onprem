{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest\n",
    "\n",
    "> Core functionality for `onprem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
    "        )\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_documents(source_directory:str, ignored_files: List[str] = [], ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        return\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n",
    "\n",
    "def does_vectorstore_exist(persist_directory: str, embeddings: HuggingFaceEmbeddings) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    if not db.get()['documents']:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from onprem.utils import get_datadir\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "DEFAULT_DB = 'vectordb'\n",
    "def ingest(source_directory:str, \n",
    "           embedding_model_name:str ='sentence-transformers/all-MiniLM-L6-v2',\n",
    "           embedding_model_kwargs:dict ={'device': 'cpu'}\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "    **Args**:\n",
    "    \n",
    "      - *source_directory*: path to folder containing document store\n",
    "      - *embedding_model*: name of sentence-transformers model\n",
    "      - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
    "    \n",
    "    **Returns**: `None`\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_directory):\n",
    "        raise ValueError('The source_directory does not exist.')\n",
    "    persist_directory = os.path.join(get_datadir(), DEFAULT_DB)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    chroma_settings = Settings(persist_directory=persist_directory,anonymized_telemetry=False)\n",
    "    chroma_client = chromadb.PersistentClient(settings=chroma_settings , path=persist_directory)\n",
    "    \n",
    "    texts = None\n",
    "    if does_vectorstore_exist(persist_directory, embeddings):\n",
    "        # Update and store locally vectorstore\n",
    "        print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
    "        db = Chroma(persist_directory=persist_directory, \n",
    "                    embedding_function=embeddings, \n",
    "                    client_settings=chroma_settings, client=chroma_client)\n",
    "        collection = db.get()\n",
    "        texts = process_documents(source_directory, \n",
    "                                  ignored_files=[metadata['source'] for metadata in collection['metadatas']])\n",
    "        if texts:\n",
    "            print(f\"Creating embeddings. May take some minutes...\")\n",
    "            db.add_documents(texts)\n",
    "    else:\n",
    "        # Create and store locally vectorstore\n",
    "        print(\"Creating new vectorstore\")\n",
    "        texts = process_documents(source_directory)\n",
    "        if texts:\n",
    "            print(f\"Creating embeddings. May take some minutes...\")\n",
    "            db = Chroma.from_documents(texts, \n",
    "                                       embeddings, persist_directory=persist_directory, \n",
    "                                       client_settings=chroma_settings, client=chroma_client)\n",
    "    if texts:\n",
    "        db.persist()\n",
    "        print(f\"Ingestion complete! You can now query your documents using the prompt method\")\n",
    "    db = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/ingest.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ingest\n",
       "\n",
       ">      ingest (source_directory:str, embedding_model_name:str='sentence-\n",
       ">              transformers/all-MiniLM-L6-v2',\n",
       ">              embedding_model_kwargs:dict={'device': 'cpu'})\n",
       "\n",
       "Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
       "\n",
       "**Args**:\n",
       "\n",
       "  - *source_directory*: path to folder containing document store\n",
       "  - *embedding_model*: name of sentence-transformers model\n",
       "  - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
       "\n",
       "**Returns**: `None`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/main/onprem/ingest.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ingest\n",
       "\n",
       ">      ingest (source_directory:str, embedding_model_name:str='sentence-\n",
       ">              transformers/all-MiniLM-L6-v2',\n",
       ">              embedding_model_kwargs:dict={'device': 'cpu'})\n",
       "\n",
       "Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
       "\n",
       "**Args**:\n",
       "\n",
       "  - *source_directory*: path to folder containing document store\n",
       "  - *embedding_model*: name of sentence-transformers model\n",
       "  - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
       "\n",
       "**Returns**: `None`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to load\n",
      "Ingestion complete! You can now query your documents using the prompt method\n"
     ]
    }
   ],
   "source": [
    "ingest('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
