{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnPrem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tool for running large language models on-premises using non-public data\n",
    "\n",
    "**OnPrem** is a simple Python package that makes it easier to run large language models (LLMs) on non-public or sensitive data and on machines with no internet connectivity (e.g., behind corporate firewalls). Inspired by the [privateGPT](https://github.com/imartinez/privateGPT) GitHub repo and Simon Willison's [LLM](https://pypi.org/project/llm/) command-line utility, **OnPrem** is designed to help integrate local LLMs into practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install onprem\n",
    "```\n",
    "\n",
    "For GPU support, see additional instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a file Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin in /home/amaiya/onprem_data.\n",
      " Do you want to still download it? (Y/n) Y\n",
      "[██████████████████████████████████████████████████]"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "import os.path\n",
    "from onprem import LLM\n",
    "\n",
    "url = 'https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin'\n",
    "\n",
    "llm = LLM(model_name=os.path.basename(url))\n",
    "llm.download_model(url, ssl_verify=True ) # set to False if corporate firewall gives you problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prompts to the LLM to Solve Problems\n",
    "This is an example of few-shot prompting, where we provide an example of what we want the LLM to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk to Your Documents\n",
    "\n",
    "Answers are generated from the content of your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Download Some Documents to a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "import os\n",
    "if not os.path.exists: os.mkdir('/tmp/sample_data')\n",
    "!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2004.10703.pdf -O /tmp/sample_data/ktrain_paper.pdf -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Ingest the  Documents into a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from /tmp/sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 new documents from /tmp/sample_data\n",
      "Split into 57 chunks of text (max. 500 tokens each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the prompt method\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "llm.ingest('/tmp/sample_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Answer Questions About the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ktrain is an open-source machine learning (ML) toolbox that focuses on enabling automation in the ML workow, rather than trying to entirely replace human engineers with algorithms. Its goal is to provide a flexible and user-friendly way for researchers and practitioners to perform ML tasks such as data preparation, model selection, training, and evaluation, while also allowing for customization and experimentation. Overall, ktrain seeks to augment and complement human engineers rather than attempting to entirely replace them.References:\n",
      "\n",
      "1.> /tmp/sample_data/downloaded_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "2.> /tmp/sample_data/ktrain_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "3.> /tmp/sample_data/downloaded_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best ﬁt their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n",
      "\n",
      "4.> /tmp/sample_data/ktrain_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best ﬁt their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "question = \"\"\"What is ktrain?\"\"\"\n",
    "answer, docs = llm.ask(question)\n",
    "print('References:')\n",
    "for i, document in enumerate(docs):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding Up Inference Using a GPU\n",
    "\n",
    "The above example employed the use of a CPU.  \n",
    "If you have a GPU (even an older one with less VRAM), you can speed up responses.\n",
    "\n",
    "#### Step 1: Install `llama-cpp-python` with CUDABLAS support\n",
    "```shell\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir\n",
    "```\n",
    "It is important to use the specific version shown above due to library incompatibilities.\n",
    "\n",
    "#### Step 2: Use the `n_gpu_layers` argument with `LLM`\n",
    "llm = LLM(model_name=os.path.basename(url), n_gpu_layers=128)\n",
    "\n",
    "With the steps above, calls to methods like `llm.prompt` will offload computation to your GPU and speed up responses from the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
