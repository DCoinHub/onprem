{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnPrem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tool for running large language models on-premises using non-public data\n",
    "\n",
    "**OnPrem** is a simple Python package that makes it easier to run large language models (LLMs) on non-public or sensitive data and on machines with no internet connectivity (e.g., behind corporate firewalls). Inspired by the [privateGPT](https://github.com/imartinez/privateGPT) GitHub repo and Simon Willison's [LLM](https://pypi.org/project/llm/) command-line utility, **OnPrem** is designed to help integrate local LLMs into practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install onprem\n",
    "```\n",
    "\n",
    "For GPU support, see additional instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a file Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin in /home/amaiya/onprem_data. Do you want to still download it? (Y/n) Y\n",
      "[██████████████████████████████████████████████████]"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "import os.path\n",
    "from onprem import LLM\n",
    "\n",
    "url = 'https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin'\n",
    "\n",
    "llm = LLM(model_name=os.path.basename(url))\n",
    "llm.download_model(url, ssl_verify=True ) # set to False if corporate firewall gives you problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prompts to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Speed Up Inference Using a GPU\n",
    "\n",
    "The above example employed the use of a CPU.  \n",
    "If you have a GPU (even an older one with less VRAM), you can speed up responses.\n",
    "\n",
    "#### Step 1: Install `llama-cpp-python` with CUDABLAS support\n",
    "```shell\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir\n",
    "```\n",
    "It is important to use the specific version shown above due to library incompatibilities.\n",
    "\n",
    "#### Step 2: Use the `n_gpu_layers` argument with `LLM`\n",
    "llm = LLM(model_name=os.path.basename(url), n_gpu_layers=128)\n",
    "\n",
    "With the steps above, calls to methods like `llm.prompt` will offload computation to your GPU and speed up responses from the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
